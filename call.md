---
title: Call for papers
layout: default

navigation_weight: 2

---

# Topics of interest

The topics of interest include, but are not limited to: 
- Olfactory, musical and visual information extraction from text 
- Olfactory, musical and visual knowledge representation
- Resources for olfactory, musical and visual knowledge 
- Extraction of socio-cultural and historical context information from text
- Extraction of time, space, events, people and (musical, olfactory and visual) artifacts from text
- Knowledge graphs integrating  olfactory, musical and visual knowledge
- Multilingual text corpora on musical and olfactory heritage
- Automatic and semi-automatic generation of language descriptions of sensory experience 
- Methods and approaches to represent degrees of sensorial experiences (e.g. intensity, dissipation) 
- Applications for teaching/training/valorisation of multisensory data and knowledge 

# Submission
We solicit submissions of long and short papers covering a wide range of NLP and semantic web topics dealing with the extraction and analysis of information from multisensory data. Papers presenting collaborations among researchers with different backgrounds or from different research communities are particularly welcome. All submissions must be in PDF and written in English, single-blind and formatted in the style of the [OASIcs template](https://submission.dagstuhl.de/documentation/authors#) as the main [LDK 2021 conference](http://2021.ldk-conf.org/) (see the [instruction for authors](https://www.dagstuhl.de/en/publications/oasics/instructions-for-authors/)). Long papers should be between 10 and 15 pages, while short papers should include 6 to 8 pages and present a more focused contribution or a position paper.

The MDK 2021 proceedings will be published with CEUR-WS.org. The submission web site is [https://www.easychair.org/conferences/?conf=mdk2021](https://www.easychair.org/conferences/?conf=mdk2021)

For any questions, please feel free to contact the organisers directly, or via mdk2021@easychair.org.


